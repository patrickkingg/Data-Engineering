{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration and Temperature Data Modeling Project\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Combining immigration data with temperature data to create an OLAP data model in order to find correlation between immagration behavior and temerpature data.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "We will be using Spark to do all the necessary data transformation to create our immagration and temperature data warehouse. The datawarehouse will sit in the SparkSQL temp tables. We will create dimension tables using both the I94 immagration and temperature datasets. We will then join our dimension tables on valid city codes to create our fact table for analysis.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "* I94 Immigration Data comes from the US National TOURISM and Trade Office. The data is partioned by month-year and can be found here: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "\n",
    "* The temperature data is a Kaggle data set that includes temperatures in cities around the world. It can be found here: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "The code below creates a dictionary of all the valid i94ports. We determine which i94ports are valid from the I_94_SAS_Labels_Description.SAS provided in our directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_airport = {}\n",
    "with open('valid.txt') as f:\n",
    "     for line in f:\n",
    "         match = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'').search(line)\n",
    "         valid_airport[match[1]]=[match[2]]\n",
    "valid_airpot_list = list(valid_airport.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALC', 'ANC', 'BAR', 'DAC', 'PIZ', 'DTH', 'EGL', 'FRB', 'HOM', 'HYD', 'JUN', '5KE', 'KET', 'MOS', 'NIK', 'NOM', 'PKC', 'ORI', 'SKA', 'SNP', 'TKI', 'WRA', 'HSV', 'MOB', 'LIA', 'ROG', 'DOU', 'LUK', 'MAP', 'NAC', 'NOG', 'PHO', 'POR', 'SLU', 'SAS', 'TUC', 'YUI', 'AND', 'BUR', 'CAL', 'CAO', 'FRE', 'ICP', 'LNB', 'LOS', 'BFL', 'OAK', 'ONT', 'OTM', 'BLT', 'PSP', 'SAC', 'SLS', 'SDP', 'SFR', 'SNJ', 'SLO', 'SLI', 'SPC', 'SYS', 'SAA', 'STO', 'TEC', 'TRV', 'APA', 'ASE', 'COS', 'DEN', 'DRO', 'BDL', 'BGC', 'GRT', 'HAR', 'NWH', 'NWL', 'TST', 'WAS', 'DOV', 'DVD', 'WLL', 'BOC', 'SRQ', 'CAN', 'DAB', 'FRN', 'FTL', 'FMY', 'FPF', 'HUR', 'GNV', 'JAC', 'KEY', 'LEE', 'MLB', 'MIA', 'APF', 'OPF', 'ORL', 'PAN', 'PEN', 'PCF', 'PEV', 'PSJ', 'SFB', 'SGJ', 'SAU', 'FPR', 'SPE', 'TAM', 'WPB', 'ATL', 'BRU', 'AGS', 'SAV', 'AGA', 'HHW', 'OGG', 'KOA', 'LIH', 'CID', 'DSM', 'BOI', 'EPI', 'IDA', 'PTL', 'SPI', 'CHI', 'DPA', 'PIA', 'RFD', 'UGN', 'GAR', 'HMM', 'INP', 'MRL', 'SBN', 'ICT', 'LEX', 'LOU', 'BTN', 'LKC', 'LAK', 'MLU', 'MGC', 'NOL', 'BOS', 'GLO', 'BED', 'LYN', 'ADW', 'BAL', 'MKG', 'PAX', 'BGM', 'BOO', 'BWM', 'BCK', 'CLS', 'CRB', 'COB', 'EST', 'EPT', 'EPM', 'FOR', 'FTF', 'FTK', 'HML', 'HTM', 'JKM', 'KAL', 'LIM', 'LUB', 'MAD', 'POM', 'RGM', 'SBR', 'SRL', 'SPA', 'VNB', 'VCB', 'AGN', 'ALP', 'BCY', 'DET', 'GRP', 'GRO', 'ISL', 'MRC', 'MRY', 'PTK', 'PHU', 'RBT', 'SAG', 'SSM', 'SCL', 'YIP', 'BAU', 'CAR', 'GTF', 'INL', 'CRA', 'MIC', 'DUL', 'ELY', 'GPM', 'SVC', \"INT'\\t=\\t'INT\", 'LAN', 'MSP', 'LIN', 'NOY', 'PIN', '48Y', 'RAN', 'RST', 'ROS', 'SPM', 'WSB', 'WAR', 'KAN', 'SGF', 'STL', 'WHI', 'WHM', 'GPT', 'GTR', 'GUL', 'PAS', 'JAN', 'BIL', 'BTM', 'CHF', 'CTB', 'CUT', 'DLB', 'EUR', 'BZN', 'FCA', 'GGW', 'GRE', 'HVR', 'HEL', 'LWT', 'MGM', 'OPH', 'PIE', 'RAY', 'ROO', 'SCO', 'SWE', 'TRL', 'TUR', 'WCM', 'CLT', 'FAY', 'MRH', 'FOP', 'GSO', 'RDU', 'SSC', 'WIL', 'AMB', 'ANT', 'CRY', 'DNS', 'FAR', 'FRT', 'GRF', 'HNN', 'HNS', 'MAI', 'MND', 'NEC', 'NOO', 'NRG', 'PEM', 'SAR', 'SHR', 'SJO', 'WAL', 'WHO', 'WND', 'OMA', 'LEB', 'MHT', 'PNH', 'PSM', 'BYO', 'CNJ', 'HOB', 'JER', 'WRI', 'MMU', 'NEW', 'PER', 'ACY', 'ALA', 'ABQ', 'ANP', 'CRL', 'COL', 'CDD', 'DNM', 'LAS', 'LOB', 'RUI', 'STR', 'RNO', 'FLX', 'LVG', 'REN', 'ALB', 'AXB', 'BUF', 'CNH', 'CAP', 'CHM', 'CHT', 'CLA', 'FTC', 'LAG', 'LEW', 'MAS', 'MAG', 'MOO', 'MRR', 'NYC', 'NIA', 'OGD', 'OSW', 'ELM', 'ROC', 'ROU', 'SWF', 'SYR', 'THO', 'TRO', 'WAT', 'HPN', 'WRB', 'YOU', 'AKR', 'ATB', 'CIN', 'CLE', 'CLM', 'LOR', 'MBO', 'SDY', 'TOL', 'OKC', 'TUL', 'AST', 'COO', 'HIO', 'MED', 'NPT', 'POO', 'PUT', 'RDM', 'ERI', 'MDT', 'HSB', 'PHI', 'PIT', 'AGU', 'BQN', 'JCP', 'ENS', 'FAJ', 'HUM', 'JOB', 'MAY', 'PON', 'PSE', 'SAJ', 'VQS', 'PRO', 'PVD', 'CHL', 'CAE', 'GEO', 'GSP', 'GRR', 'MYR', 'SPF', 'HON', 'SAI', 'TYS', 'MEM', 'NSV', 'TRI', 'ADS', 'ADT', 'ANZ', 'AUS', 'BEA', 'BBP', 'SCC', 'BTC', 'BOA', 'BRO', 'CRP', 'DAL', 'DLR', 'DNA', 'EGP', 'ELP', 'FAB', 'FAL', 'FTH', 'AFW', 'FPT', 'GAL', 'HLG', 'HID', 'HOU', 'SGR', 'LLB', 'LCB', 'LRN', 'LAR', 'LSE', 'IND', 'LOI', 'MRS', 'MCA', 'MAF', 'PDN', 'PBB', 'PHR', 'PAR', 'ISB', 'POE', 'PRE', 'PGR', 'RIO', 'ROM', 'SNA', 'SNN', 'VIB', 'YSL', 'CHA', 'CHR', 'CRU', 'FRK', 'STT', 'LGU', 'SLC', 'CHO', 'DAA', 'HOP', 'HEF', 'NWN', 'NOR', 'RCM', 'ABS', 'ABG', 'BEB', 'BEE', 'BRG', 'CNA', 'DER', 'DLV', 'ERC', 'HIG', 'MOR', 'NPV', 'NRT', 'NRN', 'PIV', 'RIF', 'STA', 'SWB', 'WBE', 'ABE', 'ANA', 'BEL', 'BLI', 'BLA', 'BWA', 'CUR', 'DVL', 'EVE', 'FER', 'FRI', 'FWA', 'KLM', 'LAU', 'LON', 'MET', 'MWH', 'NEA', 'NIG', 'OLY', 'ORO', 'PWB', 'PIR', 'PNG', 'PTO', 'SEA', 'SPO', 'SUM', 'TAC', 'PSC', 'VAN', 'AGM', 'BAY', 'GRB', 'MNW', 'MIL', 'MSN', 'CHS', 'CLK', 'BLF', 'CSP', 'CLG', 'EDA', 'YHC', 'HAL', 'MON', 'OTT', 'YXE', 'TOR', 'VCV', 'VIC', 'WIN', 'AMS', 'ARB', 'BAN', 'BEI', 'PEK', 'BDA', 'BOG', 'EZE', 'CUN', 'CRQ', 'MVD', 'DUB', 'FOU', 'FBA', 'MTY', 'HMO', 'GCM', 'GDL', 'HAM', 'ICN', 'IWA', 'CND', 'LAH', 'DUR', 'MAL', 'MDE', 'MEX', 'LHR', 'NBO', 'NAS', 'NCA', 'PTY', 'SPV', 'UIO', 'RIT', 'SNO', 'SLP', 'SAN', 'SRO', 'GRU', 'SHA', 'HIL', 'TOK', 'VER', 'LGW', 'ZZZ', 'CHN', 'CNC', 'MAA', 'AG0', 'BHM', 'BHX', 'CAK', 'FOK', 'LND', 'MAR', 'MLI', 'RIV', 'RME', 'VNY', 'YUM']\n"
     ]
    }
   ],
   "source": [
    "print(valid_airpot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp_pd = pd.read_csv(fname, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "The code below creates a dictionary of all the valid i94ports. We determine which i94ports are valid from the I_94_SAS_Labels_Description.SAS provided in our directory.\n",
    "\n",
    "For the temperature data set we will have to create a new column called i94port which we will use to join back to our i94 dataset. We will also need to drop duplicates in this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALC', 'ANC', 'BAR', 'DAC', 'PIZ', 'DTH', 'EGL', 'FRB', 'HOM', 'HYD', 'JUN', '5KE', 'KET', 'MOS', 'NIK', 'NOM', 'PKC', 'ORI', 'SKA', 'SNP', 'TKI', 'WRA', 'HSV', 'MOB', 'LIA', 'ROG', 'DOU', 'LUK', 'MAP', 'NAC', 'NOG', 'PHO', 'POR', 'SLU', 'SAS', 'TUC', 'YUI', 'AND', 'BUR', 'CAL', 'CAO', 'FRE', 'ICP', 'LNB', 'LOS', 'BFL', 'OAK', 'ONT', 'OTM', 'BLT', 'PSP', 'SAC', 'SLS', 'SDP', 'SFR', 'SNJ', 'SLO', 'SLI', 'SPC', 'SYS', 'SAA', 'STO', 'TEC', 'TRV', 'APA', 'ASE', 'COS', 'DEN', 'DRO', 'BDL', 'BGC', 'GRT', 'HAR', 'NWH', 'NWL', 'TST', 'WAS', 'DOV', 'DVD', 'WLL', 'BOC', 'SRQ', 'CAN', 'DAB', 'FRN', 'FTL', 'FMY', 'FPF', 'HUR', 'GNV', 'JAC', 'KEY', 'LEE', 'MLB', 'MIA', 'APF', 'OPF', 'ORL', 'PAN', 'PEN', 'PCF', 'PEV', 'PSJ', 'SFB', 'SGJ', 'SAU', 'FPR', 'SPE', 'TAM', 'WPB', 'ATL', 'BRU', 'AGS', 'SAV', 'AGA', 'HHW', 'OGG', 'KOA', 'LIH', 'CID', 'DSM', 'BOI', 'EPI', 'IDA', 'PTL', 'SPI', 'CHI', 'DPA', 'PIA', 'RFD', 'UGN', 'GAR', 'HMM', 'INP', 'MRL', 'SBN', 'ICT', 'LEX', 'LOU', 'BTN', 'LKC', 'LAK', 'MLU', 'MGC', 'NOL', 'BOS', 'GLO', 'BED', 'LYN', 'ADW', 'BAL', 'MKG', 'PAX', 'BGM', 'BOO', 'BWM', 'BCK', 'CLS', 'CRB', 'COB', 'EST', 'EPT', 'EPM', 'FOR', 'FTF', 'FTK', 'HML', 'HTM', 'JKM', 'KAL', 'LIM', 'LUB', 'MAD', 'POM', 'RGM', 'SBR', 'SRL', 'SPA', 'VNB', 'VCB', 'AGN', 'ALP', 'BCY', 'DET', 'GRP', 'GRO', 'ISL', 'MRC', 'MRY', 'PTK', 'PHU', 'RBT', 'SAG', 'SSM', 'SCL', 'YIP', 'BAU', 'CAR', 'GTF', 'INL', 'CRA', 'MIC', 'DUL', 'ELY', 'GPM', 'SVC', \"INT'\\t=\\t'INT\", 'LAN', 'MSP', 'LIN', 'NOY', 'PIN', '48Y', 'RAN', 'RST', 'ROS', 'SPM', 'WSB', 'WAR', 'KAN', 'SGF', 'STL', 'WHI', 'WHM', 'GPT', 'GTR', 'GUL', 'PAS', 'JAN', 'BIL', 'BTM', 'CHF', 'CTB', 'CUT', 'DLB', 'EUR', 'BZN', 'FCA', 'GGW', 'GRE', 'HVR', 'HEL', 'LWT', 'MGM', 'OPH', 'PIE', 'RAY', 'ROO', 'SCO', 'SWE', 'TRL', 'TUR', 'WCM', 'CLT', 'FAY', 'MRH', 'FOP', 'GSO', 'RDU', 'SSC', 'WIL', 'AMB', 'ANT', 'CRY', 'DNS', 'FAR', 'FRT', 'GRF', 'HNN', 'HNS', 'MAI', 'MND', 'NEC', 'NOO', 'NRG', 'PEM', 'SAR', 'SHR', 'SJO', 'WAL', 'WHO', 'WND', 'OMA', 'LEB', 'MHT', 'PNH', 'PSM', 'BYO', 'CNJ', 'HOB', 'JER', 'WRI', 'MMU', 'NEW', 'PER', 'ACY', 'ALA', 'ABQ', 'ANP', 'CRL', 'COL', 'CDD', 'DNM', 'LAS', 'LOB', 'RUI', 'STR', 'RNO', 'FLX', 'LVG', 'REN', 'ALB', 'AXB', 'BUF', 'CNH', 'CAP', 'CHM', 'CHT', 'CLA', 'FTC', 'LAG', 'LEW', 'MAS', 'MAG', 'MOO', 'MRR', 'NYC', 'NIA', 'OGD', 'OSW', 'ELM', 'ROC', 'ROU', 'SWF', 'SYR', 'THO', 'TRO', 'WAT', 'HPN', 'WRB', 'YOU', 'AKR', 'ATB', 'CIN', 'CLE', 'CLM', 'LOR', 'MBO', 'SDY', 'TOL', 'OKC', 'TUL', 'AST', 'COO', 'HIO', 'MED', 'NPT', 'POO', 'PUT', 'RDM', 'ERI', 'MDT', 'HSB', 'PHI', 'PIT', 'AGU', 'BQN', 'JCP', 'ENS', 'FAJ', 'HUM', 'JOB', 'MAY', 'PON', 'PSE', 'SAJ', 'VQS', 'PRO', 'PVD', 'CHL', 'CAE', 'GEO', 'GSP', 'GRR', 'MYR', 'SPF', 'HON', 'SAI', 'TYS', 'MEM', 'NSV', 'TRI', 'ADS', 'ADT', 'ANZ', 'AUS', 'BEA', 'BBP', 'SCC', 'BTC', 'BOA', 'BRO', 'CRP', 'DAL', 'DLR', 'DNA', 'EGP', 'ELP', 'FAB', 'FAL', 'FTH', 'AFW', 'FPT', 'GAL', 'HLG', 'HID', 'HOU', 'SGR', 'LLB', 'LCB', 'LRN', 'LAR', 'LSE', 'IND', 'LOI', 'MRS', 'MCA', 'MAF', 'PDN', 'PBB', 'PHR', 'PAR', 'ISB', 'POE', 'PRE', 'PGR', 'RIO', 'ROM', 'SNA', 'SNN', 'VIB', 'YSL', 'CHA', 'CHR', 'CRU', 'FRK', 'STT', 'LGU', 'SLC', 'CHO', 'DAA', 'HOP', 'HEF', 'NWN', 'NOR', 'RCM', 'ABS', 'ABG', 'BEB', 'BEE', 'BRG', 'CNA', 'DER', 'DLV', 'ERC', 'HIG', 'MOR', 'NPV', 'NRT', 'NRN', 'PIV', 'RIF', 'STA', 'SWB', 'WBE', 'ABE', 'ANA', 'BEL', 'BLI', 'BLA', 'BWA', 'CUR', 'DVL', 'EVE', 'FER', 'FRI', 'FWA', 'KLM', 'LAU', 'LON', 'MET', 'MWH', 'NEA', 'NIG', 'OLY', 'ORO', 'PWB', 'PIR', 'PNG', 'PTO', 'SEA', 'SPO', 'SUM', 'TAC', 'PSC', 'VAN', 'AGM', 'BAY', 'GRB', 'MNW', 'MIL', 'MSN', 'CHS', 'CLK', 'BLF', 'CSP', 'CLG', 'EDA', 'YHC', 'HAL', 'MON', 'OTT', 'YXE', 'TOR', 'VCV', 'VIC', 'WIN', 'AMS', 'ARB', 'BAN', 'BEI', 'PEK', 'BDA', 'BOG', 'EZE', 'CUN', 'CRQ', 'MVD', 'DUB', 'FOU', 'FBA', 'MTY', 'HMO', 'GCM', 'GDL', 'HAM', 'ICN', 'IWA', 'CND', 'LAH', 'DUR', 'MAL', 'MDE', 'MEX', 'LHR', 'NBO', 'NAS', 'NCA', 'PTY', 'SPV', 'UIO', 'RIT', 'SNO', 'SLP', 'SAN', 'SRO', 'GRU', 'SHA', 'HIL', 'TOK', 'VER', 'LGW', 'ZZZ', 'CHN', 'CNC', 'MAA', 'AG0', 'BHM', 'BHX', 'CAK', 'FOK', 'LND', 'MAR', 'MLI', 'RIV', 'RME', 'VNY', 'YUM']\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "valid_airport = {}\n",
    "with open('valid.txt') as file:\n",
    "     for line in file:\n",
    "         match = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'').search(line)\n",
    "         valid_airport[match[1]]=[match[2]]\n",
    "valid_airpot_list = list(valid_airport.keys())\n",
    "print(valid_airpot_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94port|\n",
      "+-------+\n",
      "|    ATL|\n",
      "|    WAS|\n",
      "|    NYC|\n",
      "|    NYC|\n",
      "|    NYC|\n",
      "|    NYC|\n",
      "|    NYC|\n",
      "|    NYC|\n",
      "|    NYC|\n",
      "|    NYC|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clean immigration data\n",
    "fpath = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_im = spark.read.format('com.github.saurfang.sas.spark').load(fpath)\n",
    "df_im_clean = df_im.filter(col('i94port').isin(valid_airpot_list))\n",
    "df_im_clean.select('i94port').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "@udf()\n",
    "def convert_i94port_code(city):\n",
    "    for key in valid_airport:\n",
    "        if city.lower() in valid_airport[key][0].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANCHORAGE, AK         \n"
     ]
    }
   ],
   "source": [
    "print(valid_airport['ANC'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_temp_spark = spark.createDataFrame(df_temp_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7bd891aa025d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_temp_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df_temp_spark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a7ec566878eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_temp_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_temp_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i94port'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_i94port_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_temp_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_temp_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i94port'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_temp_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i94port'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'City'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#clean temp data\n",
    "ftemp_path = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = spark.read.format(\"csv\").option(\"header\", \"true\").load(ftemp_path)\n",
    "df_temp_clean = df_temp.where(col('AverageTemperature') != 'NaN')\n",
    "df_temp_clean = df_temp_clean.dropDuplicates(['Country', 'City'])\n",
    "df_temp_clean = df_temp_clean.withColumn('i94port', convert_i94port_code(df_temp_clean.City))\n",
    "df_temp_clean = df_temp_clean.filter(col('i94port') != 'null')\n",
    "# df_temp_clean.select('i94port','City').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "#### I94 Immigration Dimension Table\n",
    "* i94_year = year\n",
    "* i94_month = month\n",
    "* i94_city = origin city code\n",
    "* i94port = destination city code\n",
    "* arrival_date = arrival date\n",
    "* depart_date = departure date\n",
    "* i94_visa = reason for immigration\n",
    "\n",
    "#### Temperature Dimension Table\n",
    "* i94port = destination city code\n",
    "* AverageTemperature = average temperature\n",
    "* City = city name\n",
    "* Country = country name\n",
    "\n",
    "#### Fact Table (I94 Immigration and Temperature Dimention Tables joined on i94port column)\n",
    "* i94_year = year\n",
    "* i94_month = month\n",
    "* i94_city = origin city code\n",
    "* i94port = destination city code\n",
    "* arrival_date = arrival date\n",
    "* depart_date = departure date\n",
    "* i94_visa = reason for immigration\n",
    "* AverageTemperature = average temperature\n",
    "* City = city name\n",
    "* Country = country name\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Read I94 immigration and Temperature Data into  spark dataframe\n",
    "2. Clean I94 immigration and Temperature Data\n",
    "3. Create I94 immigration and Temperature dimesion tables\n",
    "4. Create Immigration/Temperature Fact table by joining the two dimension tables created\n",
    "5. Perform Data Quality checks for each table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. Read I94 immigration and Temperature Data into  spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read I94 into spark dataframe\n",
    "fpath_im = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_im_spark = spark.read.format('com.github.saurfang.sas.spark').load(fpath_im)\n",
    "\n",
    "# read Temperature into spark dataframe\n",
    "fpath_temp = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp_spark = spark.read.format(\"csv\").option(\"header\", \"true\").load(fpath_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. Clean I94 immigration and Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean I94 dataframe \n",
    "valid_airport = {}\n",
    "with open('valid.txt') as file:\n",
    "     for line in file:\n",
    "         match = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'').search(line)\n",
    "         valid_airport[match[1]]=[match[2]]\n",
    "            \n",
    "valid_airpot_list = list(valid_airport.keys())\n",
    "df_im_clean = df_im_spark.filter(col('i94port').isin(valid_airpot_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#UDF to convert city name into i94port code\n",
    "@udf()\n",
    "def convert_i94port_code(city):\n",
    "    for key in valid_airport:\n",
    "        if city.lower() in valid_airport[key][0].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf()\n",
    "def convert_i94port_code(city):\n",
    "    for key in valid_airport:\n",
    "        if city.lower() in valid_airport[key][0].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean Temperature dataframe\n",
    "df_temp_clean = df_temp_spark.where(col('AverageTemperature') != 'NaN')\n",
    "df_temp_clean = df_temp_clean.dropDuplicates(['Country', 'City'])\n",
    "df_temp_clean = df_temp_clean.withColumn('i94port', convert_i94port_code(df_temp_clean.City))\n",
    "df_temp_clean = df_temp_clean.filter(col('i94port') != 'null')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. Create I94 immigration and Temperature dimesion tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "im_dim = df_im_clean.select([\"i94yr\", \"i94mon\", \"i94cit\", \"i94port\", \"arrdate\", \"depdate\", \"i94visa\"])\n",
    "temp_dim = df_temp_clean.select([\"i94port\", \"AverageTemperature\", \"City\", \"Country\"])\n",
    "\n",
    "im_dim.createOrReplaceTempView(\"i94_immigration\")\n",
    "temp_dim.createOrReplaceTempView(\"temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "4. Create Immigration/Temperature Fact table by joining the two dimension tables created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_temp_fact = spark.sql(\"\"\"\n",
    "\n",
    "SELECT i94_immigration.i94yr as i94_year,\n",
    "        i94_immigration.i94mon as i94_month,\n",
    "        i94_immigration.cit as i94_city,\n",
    "        i94_immigration.port as i94port,\n",
    "        i94_immigration.arrdate as arrival_date,\n",
    "        i94_immigration.depdate as depart_date,\n",
    "        i94_immigration.i94visa as i94_visa, \n",
    "        temperature.AverageTemperature as AverageTemperature,\n",
    "        temperature.City as city,\n",
    "        temperature.Country as country\n",
    "\n",
    "FROM i94_immigration\n",
    "INNER JOIN temperature\n",
    "ON i94_immigration.i94port = temperature.i94port\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_temp_fact.write.mode('append').partitionBy('i94port').parquet('/res/i94_temp_fact.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_quality(df):\n",
    "    df_count = df.cache.count()\n",
    "    if df_count != 0:\n",
    "        print('Data quality passed! > 0 records')\n",
    "    else:\n",
    "        print('Data quality failed! = 0 records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_quality(im_dim)\n",
    "data_quality(temp_dim)\n",
    "data_quality(i94_temp_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "#### I94 Immigration Dimension Table\n",
    "* i94_year = year\n",
    "* i94_month = month\n",
    "* i94_city = origin city code\n",
    "* i94port = destination city code\n",
    "* arrival_date = arrival date\n",
    "* depart_date = departure date\n",
    "* i94_visa = reason for immigration\n",
    "\n",
    "#### Temperature Dimension Table\n",
    "* i94port = destination city code\n",
    "* AverageTemperature = average temperature\n",
    "* City = city name\n",
    "* Country = country name\n",
    "\n",
    "#### Fact Table (I94 Immigration and Temperature Dimention Tables joined on i94port column)\n",
    "* i94_year = year\n",
    "* i94_month = month\n",
    "* i94_city = origin city code\n",
    "* i94port = destination city code\n",
    "* arrival_date = arrival date\n",
    "* depart_date = departure date\n",
    "* i94_visa = reason for immigration\n",
    "* AverageTemperature = average temperature\n",
    "* City = city name\n",
    "* Country = country name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "We used Spark to do our transformation, because it has the ability to handle many types of data files. In this proejct we are using SAS, txt, csv, and parquet file types. We used Spark SQL as our SQL interface powered by the Spark engine to create our datawarehouse. \n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "Data is formatted at the month-year level. That means the data should be updated at a monthly batch level.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " 1. The data was increased by 100x.\n",
    "     * use data warehouse technology like redshift or snowflake. Benefits of snowflake is that the compute and storage is decoupled (optimizes performance and cost). data warehouse enables us to do heavier workloads/transformations.\n",
    "     \n",
    " 2. The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     * use airflow as an orchestrator to trigger daily batch jobs at 7am. \n",
    " 3. The database needed to be accessed by 100+ people.\n",
    "     * use data warehouse technology like redshift or snowflake. Benefits of snowflake is that the compute and storage is decoupled (optimizes performance and cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
